# WordGPT

It's an implementation from scratch of a _decoder only_ transformer model, with a GPT-like architecture and custom word-based tokenizer. The model is trained on the [WikiText-2 dataset](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/).
